{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinFarres/transformers_Chatbot_QA/blob/main/transformers_Chatbot_Q%26Av2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Q&A - Seq2Seq Transformer Model"
      ],
      "metadata": {
        "id": "XM2CzRxtzFyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ],
      "metadata": {
        "id": "6mLgqYfuzQ3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Mxi7KevYyE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: IMPROVED DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class DialogDataset(Dataset):\n",
        "    def __init__(self, file_path, vocab=None, max_len=50):\n",
        "        self.pairs = []\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    question, answer = parts\n",
        "                    self.pairs.append((question.lower(), answer.lower()))\n",
        "\n",
        "\n",
        "        # Build or use existing vocabulary\n",
        "        if vocab is None:\n",
        "            self.vocab = self.build_vocab()\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "    def build_vocab(self):\n",
        "\n",
        "        words = []\n",
        "        for q, a in self.pairs:\n",
        "            words.extend(self.tokenize(q))\n",
        "            words.extend(self.tokenize(a))\n",
        "\n",
        "        word_counts = Counter(words)\n",
        "\n",
        "        vocab = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        vocab.extend([word for word, count in word_counts.most_common() if count >= 3])\n",
        "\n",
        "        # Cap vocabulary size\n",
        "        MAX_VOCAB_SIZE = 5000\n",
        "        if len(vocab) > MAX_VOCAB_SIZE:\n",
        "            vocab = vocab[:MAX_VOCAB_SIZE]\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = re.sub(r\"[^\\w\\s']\", '', text)\n",
        "        return text.split()\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = self.tokenize(text)\n",
        "        indices = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in tokens]\n",
        "        return indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.pairs[idx]\n",
        "\n",
        "        src = [self.word2idx['<SOS>']] + self.encode(question) + [self.word2idx['<EOS>']]\n",
        "        tgt = [self.word2idx['<SOS>']] + self.encode(answer) + [self.word2idx['<EOS>']]\n",
        "\n",
        "        src = src[:self.max_len]\n",
        "        tgt = tgt[:self.max_len]\n",
        "\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in batch:\n",
        "        src_batch.append(src)\n",
        "        tgt_batch.append(tgt)\n",
        "\n",
        "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
        "    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
        "\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DialogDataset('dialogs.txt')\n",
        "\n",
        "print(f\"Number of dialog pairs: {len(dataset)}\")\n",
        "print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
        "print(f\"First pair (encoded): {dataset[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "asO4YQlvxEZq",
        "outputId": "1d540e44-3f64-4325-82eb-8b00f63320f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dialogs.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4083194315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDialogDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dialogs.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of dialog pairs: {len(dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Vocabulary size: {len(dataset.vocab)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"First pair (encoded): {dataset[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3260557219.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, vocab, max_len)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dialogs.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"hi, how are you doing?\"\n",
        "tokens = dataset.tokenize(input)\n",
        "encode = dataset.encode(input)\n",
        "encode.insert(0, dataset.word2idx['<SOS>'])\n",
        "encode.append(dataset.word2idx['<EOS>'])\n",
        "print(f\"Input : {input}\")\n",
        "print(f\"Tokens : {tokens}\")\n",
        "print(f\"Encode : {encode}\")"
      ],
      "metadata": {
        "id": "H4bLAbpCx_B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Positional Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "VYbiXD7ZzWS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: POSITIONAL ENCODING\n",
        "# ============================================================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of shape (max_len, d_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        # Create the div_term for sine and cosine\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-math.log(10000.0) / d_model))\n",
        "\n",
        "\n",
        "        # Apply sine to even indices\n",
        "        # Even positions: sin(pos / 10000^(2i/d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "\n",
        "        # Apply cosine to odd indices\n",
        "        # Odd positions: cos(pos / 10000^(2i/d_model))\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "GAS21CcmzeZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embedding:      [0.2, 0.5, 0.1, ...]\n",
        "\n",
        "\\+ Positional info:   [0.1, 0.0, 0.3, ...]\n",
        "\n",
        "= Final embedding:   [0.3, 0.5, 0.4, ...]"
      ],
      "metadata": {
        "id": "leQg8LiU4owR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Transformer Model\n",
        "\n",
        "### Architecture - Transformer\n",
        "    1. Embedding layer\n",
        "    2. Positional encoding\n",
        "    3. Transformer encoder (processes input question)\n",
        "    4. Transformer decoder (generates answer)\n",
        "    5. Output linear layer\n",
        "\n",
        "### Encoder Flow\n",
        "\n",
        "    Input Question → Embedding → Positional Encoding → Multi-Head Attention → Feed Forward → Output\n",
        "\n",
        "### Decoder Flow\n",
        "    Previous Words → Embedding → Positional Encoding →\n",
        "    Masked Multi-Head Attention →\n",
        "    Cross-Attention (with Encoder output) →\n",
        "    Feed Forward →\n",
        "    Output Prediction"
      ],
      "metadata": {
        "id": "Iu9e8gF34ZvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: TRANSFORMER MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, nhead=8,\n",
        "                 num_encoder_layers=3, num_decoder_layers=3,\n",
        "                 dim_feedforward=1024, dropout=0.15, max_len=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc_out.bias.data.zero_()\n",
        "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        if isinstance(embeddings, np.ndarray):\n",
        "            embeddings = torch.from_numpy(embeddings)\n",
        "\n",
        "        if embeddings.shape[1] < self.d_model:\n",
        "            padding = torch.randn(embeddings.shape[0], self.d_model - embeddings.shape[1]) * 0.01  # Small random instead of zeros\n",
        "            embeddings = torch.cat([embeddings, padding], dim=1)\n",
        "\n",
        "        self.src_embedding.weight.data.copy_(embeddings)\n",
        "        self.tgt_embedding.weight.data.copy_(embeddings)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "    def create_padding_mask(self, seq, pad_idx=0):\n",
        "        return (seq == pad_idx)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Create masks\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "        src_padding_mask = self.create_padding_mask(src)\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt)\n",
        "\n",
        "        # Embed and add positional encoding\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        # Pass through transformer\n",
        "        output = self.transformer(\n",
        "            src_emb,\n",
        "            tgt_emb,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask\n",
        "        )\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def encode(self, src):\n",
        "        src_padding_mask = self.create_padding_mask(src)\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        memory = self.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
        "        return memory\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask=None):\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt)\n",
        "        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
        "        output = self.transformer.decoder(\n",
        "            tgt_emb,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask\n",
        "        )\n",
        "        return self.fc_out(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "fjaC6TPa4eKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(glove_file, word2idx, embedding_dim=100):\n",
        "    embeddings = np.zeros((len(word2idx), embedding_dim))\n",
        "\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "\n",
        "            if word in word2idx:\n",
        "                embeddings[word2idx[word]] = vector\n",
        "\n",
        "    return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "5Iu6WwGl-Eyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Loop - Teacher Forcing\n",
        "\n",
        "    Target: \"I am fine\"\n",
        "\n",
        "    Input to decoder:        [<SOS>, I, am]\n",
        "    Expected output:         [I, am, fine]\n",
        "    Model tries to predict:  [I, am, fine]\n",
        "\n",
        "If the model predicts incorrectly, we still feed it the correct word for the next step."
      ],
      "metadata": {
        "id": "9eFlvi3B6rnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, device, epoch, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
        "            output = model(src, tgt_input)\n",
        "            output_flat = output.reshape(-1, output.shape[-1])\n",
        "            tgt_flat = tgt_output.reshape(-1)\n",
        "            loss = criterion(output_flat, tgt_flat)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
        "                output = model(src, tgt_input)\n",
        "                output_flat = output.reshape(-1, output.shape[-1])\n",
        "                tgt_flat = tgt_output.reshape(-1)\n",
        "                loss = criterion(output_flat, tgt_flat)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "SF0iLWt56ws_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generating Responses"
      ],
      "metadata": {
        "id": "FEBoofkh7fD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_diverse(model, src, dataset, max_len=20, device='cpu'):\n",
        "    \"\"\"NEW: Generate with diversity penalty to avoid repetitive responses\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    src_tokens = [dataset.word2idx['<SOS>']] + dataset.encode(src) + [dataset.word2idx['<EOS>']]\n",
        "    src_tensor = torch.tensor([src_tokens]).to(device)\n",
        "    memory = model.encode(src_tensor)\n",
        "\n",
        "    tgt_tokens = [dataset.word2idx['<SOS>']]\n",
        "    recent_tokens = set()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_len):\n",
        "            tgt_tensor = torch.tensor([tgt_tokens]).to(device)\n",
        "            tgt_mask = model.generate_square_subsequent_mask(len(tgt_tokens)).to(device)\n",
        "\n",
        "            output = model.decode(tgt_tensor, memory, tgt_mask)\n",
        "            logits = output[:, -1, :]\n",
        "\n",
        "            # Penalize recently used tokens\n",
        "            for token in recent_tokens:\n",
        "                logits[0, token] -= 2.0\n",
        "\n",
        "            # Use nucleus sampling\n",
        "            logits = logits / 0.85  # temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "            mask = cumulative_probs <= 0.9\n",
        "            mask[..., 0] = True\n",
        "\n",
        "            filtered_probs = sorted_probs.clone()\n",
        "            filtered_probs[~mask] = 0\n",
        "            filtered_probs = filtered_probs / filtered_probs.sum()\n",
        "\n",
        "            next_token_idx = torch.multinomial(filtered_probs, num_samples=1).item()\n",
        "            next_token = sorted_indices[0, next_token_idx].item()\n",
        "\n",
        "            if next_token == dataset.word2idx['<EOS>']:\n",
        "                break\n",
        "\n",
        "            tgt_tokens.append(next_token)\n",
        "            recent_tokens.add(next_token)\n",
        "            if len(recent_tokens) > 5:\n",
        "                recent_tokens = set(list(recent_tokens)[-5:])\n",
        "\n",
        "    response_tokens = []\n",
        "    for idx in tgt_tokens[1:]:\n",
        "        word = dataset.idx2word.get(idx, '<UNK>')\n",
        "        if word not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n",
        "            response_tokens.append(word)\n",
        "\n",
        "    return ' '.join(response_tokens) if response_tokens else \"...\""
      ],
      "metadata": {
        "id": "6Uhy5wPq7k4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_beam(model, src, dataset, beam_width=5, max_len=100, device='cpu'):\n",
        "    model.eval()\n",
        "    eos_idx = dataset.word2idx['<EOS>']\n",
        "    sos_idx = dataset.word2idx['<SOS>']\n",
        "\n",
        "    src_tokens = [sos_idx] + dataset.encode(src) + [eos_idx]\n",
        "    src_tensor = torch.tensor([src_tokens]).to(device)\n",
        "    memory = model.encode(src_tensor)\n",
        "\n",
        "    # Initialize beam\n",
        "    beam = [{'tokens': [sos_idx], 'score': 0.0, 'prev_scores': []}]\n",
        "\n",
        "    for step in range(max_len):\n",
        "        candidates = []\n",
        "        for b in beam:\n",
        "            if b['tokens'][-1] == eos_idx:\n",
        "                candidates.append(b)\n",
        "                continue\n",
        "\n",
        "            tgt_tensor = torch.tensor([b['tokens']]).to(device)\n",
        "            tgt_mask = model.generate_square_subsequent_mask(tgt_tensor.size(1)).to(device)\n",
        "\n",
        "            output = model.decode(tgt_tensor, memory, tgt_mask)\n",
        "            log_probs = F.log_softmax(output[:, -1, :], dim=-1)\n",
        "\n",
        "            top_log_probs, top_indices = torch.topk(log_probs, beam_width, dim=-1)\n",
        "            for i in range(beam_width):\n",
        "                next_token = top_indices[0, i].item()\n",
        "                log_prob = top_log_probs[0, i].item()\n",
        "                new_tokens = b['tokens'] + [next_token]\n",
        "                # Length-normalized score to avoid short sequences\n",
        "                new_score = (b['score'] * len(b['tokens']) + log_prob) / len(new_tokens)**0.6\n",
        "                candidates.append({'tokens': new_tokens, 'score': new_score, 'prev_scores': b['prev_scores'] + [log_prob]})\n",
        "\n",
        "        # Select top beam_width\n",
        "        beam = sorted(candidates, key=lambda x: x['score'], reverse=True)[:beam_width]\n",
        "\n",
        "        if all(b['tokens'][-1] == eos_idx for b in beam):\n",
        "            break\n",
        "\n",
        "    # Best sequence\n",
        "    best_tokens = beam[0]['tokens'][1:]\n",
        "    if best_tokens and best_tokens[-1] == eos_idx:\n",
        "        best_tokens = best_tokens[:-1]\n",
        "\n",
        "    response = ' '.join(dataset.idx2word.get(idx, '<UNK>') for idx in best_tokens\n",
        "                        if idx not in [dataset.word2idx['<PAD>'], sos_idx, eos_idx, dataset.word2idx['<UNK>']])\n",
        "    return response if response else \"...\""
      ],
      "metadata": {
        "id": "FOuHKCuL6rRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Run the Model"
      ],
      "metadata": {
        "id": "uYLcAGZL8REY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128  # Increased, monitor GPU memory\n",
        "EPOCHS = 50  # Increased\n",
        "LEARNING_RATE = 0.0002  # Reduced\n",
        "D_MODEL = 256  # Increased\n",
        "NHEAD = 8\n",
        "NUM_ENCODER_LAYERS = 3  # Increased\n",
        "NUM_DECODER_LAYERS = 3  # Increased\n",
        "DIM_FEEDFORWARD = 1024  # Increased\n",
        "DROPOUT = 0.15  # Reduced\n",
        "MAX_LEN = 100  # Increased\n",
        "BEAM_WIDTH = 5  # For new generation\n",
        "GRAD_CLIP = 0.5\n",
        "LABEL_SMOOTHING = 0.05  # Reduced\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = DialogDataset('dialogs.txt', max_len=MAX_LEN)\n",
        "\n",
        "# Split 80/20 instead of 70/30\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size], generator=generator\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain size: {train_size}, Val size: {val_size}\")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,  # Changed from 2 to 0 for compatibility\n",
        "    pin_memory=True if device.type == 'cuda' else False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if device.type == 'cuda' else False\n",
        ")\n",
        "\n",
        "# Create SMALLER model\n",
        "model = Seq2SeqTransformer(\n",
        "    vocab_size=len(dataset.vocab),\n",
        "    d_model=D_MODEL,\n",
        "    nhead=NHEAD,\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    dim_feedforward=DIM_FEEDFORWARD,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=MAX_LEN\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Load GloVe embeddings (use 100d, closest to our D_MODEL=128)\n",
        "print(\"\\nLoading GloVe embeddings...\")\n",
        "glove_embeddings = load_glove_embeddings(\n",
        "    './drive/MyDrive/glove.6B.200d.txt',  # CHANGED: Use 100d instead of 300d\n",
        "    dataset.word2idx,\n",
        "    embedding_dim=200\n",
        ")\n",
        "\n",
        "# Adjust embeddings to match D_MODEL\n",
        "if glove_embeddings.shape[1] != D_MODEL:\n",
        "    padding = np.zeros((glove_embeddings.shape[0], D_MODEL - glove_embeddings.shape[1]))\n",
        "    glove_embeddings = np.concatenate([glove_embeddings, padding], axis=1)\n",
        "\n",
        "model.load_pretrained_embeddings(glove_embeddings)\n",
        "\n",
        "# Criterion with label smoothing\n",
        "pad_idx = dataset.word2idx.get('<PAD>', 0)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    betas=(0.9, 0.98)\n",
        ")\n",
        "\n",
        "# Scheduler\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "#     optimizer,\n",
        "#     T_0=5,\n",
        "#     T_mult=2,\n",
        "#     eta_min=1e-6\n",
        "# )\n",
        "# New Scheduler with Warmup (replace CosineAnnealing)\n",
        "class InverseSqrtScheduler:\n",
        "    def __init__(self, optimizer, warmup_steps=4000, d_model=256):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d_model = d_model\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.current_step += 1\n",
        "        arg1 = self.current_step ** -0.5\n",
        "        arg2 = self.current_step * (self.warmup_steps ** -1.5)\n",
        "        lr = (self.d_model ** -0.5) * min(arg1, arg2)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "scheduler = InverseSqrtScheduler(optimizer, warmup_steps=4000, d_model=D_MODEL)\n",
        "\n",
        "# Mixed precision\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting training with OPTIMIZED hyperparameters\")\n",
        "print(\"=\"*60)\n",
        "best_val_loss = float('inf')\n",
        "patience = 6\n",
        "patience_cnt = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch, scheduler)  # Pass scheduler\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6e}\")\n",
        "\n",
        "    if val_loss < best_val_loss - 1e-4:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, 'best_chatbot_model.pt')\n",
        "        patience_cnt = 0\n",
        "        print(\"  → Model saved (new best)!\")\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        print(f\"  No improvement. patience {patience_cnt}/{patience}\")\n",
        "        if patience_cnt >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Test with diverse generation every 3 epochs\n",
        "    if (epoch + 1) % 3 == 0:\n",
        "        test_questions = [\n",
        "            \"hi, how are you doing?\",\n",
        "            \"what's your favorite movie?\",\n",
        "            \"how's the weather?\"\n",
        "        ]\n",
        "        print(\"\\n  Sample generations (Diverse Sampling):\")\n",
        "        for q in test_questions:\n",
        "            response = generate_response_beam(model, q, dataset, device=device)\n",
        "            print(f\"    Q: {q}\")\n",
        "            print(f\"    A: {response}\")\n",
        "        print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load best model\n"
      ],
      "metadata": {
        "id": "iK8q7ber8ZXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "\n",
        "checkpoint = torch.load('.best_chatbot_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "print(f\"Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "# Interactive chat\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Interactive Chat (type 'quit' to exit)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "while True:\n",
        "    user_input = __builtins__.input(\"\\nYou: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    response = generate_response_beam(model, q, dataset, device=device)\n",
        "    print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "id": "Damnz8q7544P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "f9de5593-6e03-47fa-cd39-2a3f8698dbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '.best_chatbot_model.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4156444709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.best_chatbot_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nLoaded best model from epoch {checkpoint['epoch']+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.best_chatbot_model.pt'"
          ]
        }
      ]
    }
  ]
}